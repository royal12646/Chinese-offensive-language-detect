# Chinese-offensive-language-detect
### 本作品旨在检测下面8类有害言论：
 ==涉黄有害文==
 谐音涉黄有害文本
 缩写代指有害文本（如“nmsl”→“尼玛死了”）
 谐音辱骂有害文本（如“泥马丝了”→“你妈死了”）
 辱骂有害文本（如“操你妈，你这个贱人”）
 地域攻击有害文本（如“河南人怎么嘴巴这么.....”）
 性别攻击有害文本（如“这种男的一看就很下头，天天乱搞...”）
 种族攻击有害文本（如“这些黑人好有文化哟！”）

﻿

﻿

对于国内社交媒体有害文本检测，目前开源的数据集普遍存在体量较小、覆盖范围有限的问题。尽管国外数据集数量较多，但翻译后往往不符合中文语境中的地道表达。当前可获取的中文开源数据集主要有三个：COLDataset数据集、SWSR 数据集、TOCP数据集、弱智吧数据集。

1.COLDataset 数据集

由 Deng 等人构建，包含 37,480 条标注评论，涵盖种族歧视、性别歧视和地域攻击三类内容。每条数据均带有二分类攻击性标签（攻击性 / 非攻击性）和四分类攻击标签（种族 / 地域 / 性别 / 安全）。该数据集可通过 GitHub 获取：https://github.com/thu-coai/COLDataset﻿﻿

2.TOCP数据集

该数据集是一个中文亵渎内容专项数据集，包含来自社交媒体的 16,000 余条句子和互联网爬取的 17,000 余条亵渎性表达，每条数据均标注二分类攻击性标签。需注意，该数据集的原公开渠道已失效，需通过邮件联系作者获取。﻿﻿

3.SWSR数据集

这是首个专注于中文性别歧视言论的数据集，包含 8970 条性别歧视评论和 1980 条微博性别歧视发言。该数据集附带 SexHateLex 词典——首个由辱骂性和性别相关术语构成、共 3016 条的中文词库。

4.弱智吧数据集

由于上述数据集存在正负样本比例不均的问题，我们选用了当前国内较为流行的优质数据集——弱智吧数据集作为备用正样本来源。当正样本不足时，我们从中挑选适当长度的句子补充。弱智吧数据集是从百度弱智吧收集的一系列帖子，旨在启发人们以娱乐方式使用 ChatGPT 等大模型工具，包含 8.56 万条无害高质量帖子。

 

﻿﻿ 

为了能更好的检测上述所提到的8类有害语言，本团队采用数据增强的方法解决数据体量较小、覆盖范围有限的问题：

为了增强对涉黄有害文本的检测能力，我们使用 ChatGPT 对 SexHateLex（首个由辱骂性和性别相关术语构成、共 3016 条的中文词库）词典进行数据增强。每一个短词均生成 5 条涉黄有害文本,2条安全文本，再从弱智吧中抽取21000条安全样本，构成 Aug_SexHate 数据集，共有45300个样本。我们使用 SexHateLex 和 Aug_SexHate 数据集来检测涉黄有害文本。

针对性别歧视、种族攻击和地域攻击类有害文本的检测，我们将 COLDataset 和 SWSR 数据集合并再对每个样本进行三次数据增强，构建 Aug_COSW 数据集。该数据集包含约 10 万个样本，每个样本均含有二分类标签（安全 / 攻击）及四分类标签（性别 / 种族 / 地域 / 安全）。我们基于该数据集训练模型，用于识别上述三类攻击文本。

为检测辱骂类文本与缩写代指文本（如“nmsl”表示“你妈死了”）：我们从 TOCP 数据集中提取长度小于等于 4 的短文本；然后获取其首字母，构造缩写代指版本，然后再基于缩写代指版本构建等同数量的含有缩写代指的有害文本（例如：ztmd是一群fw。）；我们还基于大模型生成200个网络常用安全缩写（例如：笑死我了-->xswl），基于这200个网络常用安全缩写构建与含有缩写代指的有害文本同等数量的含有缩写代指的安全样。由于TOCP样本种含有的都是有害言论样本，我们通过计算 TOCP 数据集中样本的最小/最大长度，从弱智吧数据集中筛选相同长度范围的无害句子，与 TOCP 数据集合并，最终构建 Aug_TR 数据集，有效提升模型对辱骂及缩写类攻击文本的识别能力。

为了检测谐音涉黄有害文本和谐音辱骂有害文本，我们在检测前首先将文本转换为拼音形式，然后通过Pinyin2Hanzi方法将拼音转换为不含谐音的文字，从而达到谐音文本的检测目的。
﻿

﻿

考虑到训练和推理成本，我们选择开源大模型hfl/chinese-macbert-base进行微调训练，基于此模型和上述3个自建数据集，我们构建了三个检测器D1（涉黄文本检测器）、D2（辱骂文本检测器）、D3（地域/种族/性别偏见检测器）。每个检测器都能检测对应的有害文本，此外，我们利用集成学习的方法，构建了一个通用检测器，如下图所示。
![image](https://github.com/user-attachments/assets/018dcd2b-4160-4dda-be6f-f8f68c1e5909)
﻿﻿﻿

我们首先将文本分别输入三个检测器，得到每个检测器预测该文本为有害文本的概率p1、p2、p3，然后各个概率乘以一个可学习的参数α1、α2、α3，再将他们的结果相加起来，如果大于0.5，则该文本被认定为有害文本，否则认定为无害文本。
