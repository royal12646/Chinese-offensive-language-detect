# Chinese-offensive-language-detect

## 环境配置
~~~
conda create -n off-detect python=3.9
conda activate off-detect
pip install requirements.txt
~~~
## 模型训练
~~~
python train.py
~~~

## 模型测试
~~~
python test.py
~~~

## Demo运行
~~~
cd ./Chinese-offensive-language-detect/Demo
sudo ufw allow 5000/tcp
python Flask/app.py

#开一个新终端
conda activate off-detect
cd ./Chinese-offensive-language-detect/Demo/User
node procedure.js

#开一个新终端
conda activate off-detect
cd ./Chinese-offensive-language-detect/Demo/
npm run dev
~~~

![image](https://github.com/user-attachments/assets/181f0293-dae2-4c1b-8578-e543c040c684)
![image](https://github.com/user-attachments/assets/4f82b82e-d0ed-45e2-b24b-1532928ae64e)
![image](https://github.com/user-attachments/assets/a288f60b-ace6-4814-837c-be441e7153be)

### 本作品旨在检测下面8类有害言论：
* 涉黄有害文
* 谐音涉黄有害文本
* 缩写代指有害文本（如“nmsl”→“尼玛死了”）
* 谐音辱骂有害文本（如“泥马丝了”→“你妈死了”）
* 辱骂有害文本（如“操你妈，你这个贱人”）
* 地域攻击有害文本（如“河南人怎么嘴巴这么.....”）
* 性别攻击有害文本（如“这种男的一看就很下头，天天乱搞...”）
* 种族攻击有害文本（如“这些黑人好有文化哟！”）

---
## 我们构造了以下数据集（数据集将在6月份开源，目前版本较久，仅供参考）
### Aug_SexHate 数据集：
    
为了增强对涉黄有害文本的检测能力，我们使用 ChatGPT 对 SexHateLex（首个由辱骂性和性别相关术语构成、共 3016 条的中文词库）词典进行数据增强。每一个短词均生成 5 条涉黄有害文本,2条安全文本，再从弱智吧中抽取21000条安全样本，构成 Aug_SexHate 数据集，共有45300个样本。我们使用 SexHateLex 和 Aug_SexHate 数据集来检测涉黄有害文本。如下图所示。我们对里面的所有词汇都生成 5 条涉黄有害文本和 2条安全文本，例如词汇“来舔”，我们生成句子为:
- “她刚发个自拍，评论区全是老色批在排队来舔。”
- “你看那谁，一副贱兮兮的样子，就等男人来舔呢。”
- “这直播明显是擦边，摆明了让粉丝花钱来舔的。”
- “她穿成这样不就是在勾人吗，一堆臭男人抢着来舔。”
- “每次她发点露骨的东西，那群死宅就屁颠屁颠跑来舔，恶心死了。”
- “小狗摇着尾巴冲过来舔我，太可爱了哈哈。”
- “这雪糕太好吃了，我都忍不住想来舔第二根。”

这样可以很好的防止模型掉入“关键词”骗圈。然后，我们从弱智吧中抽取21000条样本作为安全样本，防止样本失衡。
    
### Aug_COSW 数据集

Aug_COSW 数据集是在COLDataset和SWSR数据集的基础上对每一个样本进行3次数据增强，达到扩充数据集的效果。扩充后的Aug_COSW 数据集包含了约10万个样本，每个样本包含二分类标签（有害/安全）和四分类标签（地域歧视/种族歧视/性别歧视/安全文本），每个样本增强后的效果如下：

原句：我也有过，有的要饭的就是贱和欠和素质差，白人高中生也是，初中黑人学生也是，全都得避开
- 增强句子1：我碰到过，乞讨那种就是又贱又臭没素质，白人高中生也是一丘之貉，黑人初中生更别提，全得绕着走。
- 增强句子2：真有这种人，伸手党一个个都贱得不行，白人小年轻也一样，黑皮初中生更是极品，全他妈避雷。
- 增强句子3：经历过，什么乞丐、白人高中仔、黑人初中崽，通通一个样，烂、烦、没素质，最好永远别靠近。- 

### Aug_TR 数据集

为检测辱骂类文本与缩写代指文本（如“nmsl”表示“你妈死了”），我们从 TOCP 数据集中提取长度小于等于 4 的短文本；然后获取其首字母，构造缩写代指版本，然后再基于缩写代指版本构建等同数量的含有缩写代指的有害文本（例如：ztmd是一群fw。）；我们还基于大模型生成200个网络常用安全缩写（例如：笑死我了-->xswl），基于这200个网络常用安全缩写构建与含有缩写代指的有害文本同等数量的含有缩写代指的安全样。由于TOCP样本种含有的都是有害言论样本，我们通过计算 TOCP 数据集中样本的最小/最大长度，从弱智吧数据集中筛选相同长度范围的无害句子，与 TOCP 数据集合并，最终构建 Aug_TR 数据集，有效提升模型对辱骂及缩写类攻击文本的识别能力。

## 谐音转换方法

为了检测谐音涉黄有害文本和谐音辱骂有害文本，我们在检测前首先将文本转换为拼音形式，然后通过Pinyin2Hanzi方法将拼音转换为不含谐音的文字，从而达到谐音文本的检测目的。

## 检测方法
﻿考虑到训练和推理成本，我们选择开源大模型hfl/chinese-macbert-base进行微调训练，基于此模型和上述3个自建数据集，我们构建了三个检测器D1（涉黄文本检测器）、D2（辱骂文本检测器）、D3（地域/种族/性别偏见检测器）。每个检测器都能检测对应的有害文本，此外，我们利用集成学习的方法，构建了一个通用检测器，如下图所示。
 
我们首先将文本分别输入三个检测器，得到每个检测器预测该文本为有害文本的概率p1、p2、p3，然后各个概率乘以一个可学习的参数α1、α2、α3，再将他们的结果相加起来，如果大于0.5，则该文本被认定为有害文本，否则认定为无害文本。
![image](https://github.com/user-attachments/assets/018dcd2b-4160-4dda-be6f-f8f68c1e5909)
﻿﻿﻿
